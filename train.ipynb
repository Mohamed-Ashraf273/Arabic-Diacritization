{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8474c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a8b06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"بوَإِنْ كَانَ الْمَأْذُونُ هُوَ الْوَكِيلَ لِلْأَجْنَبِيِّ بِبَيْعِ شَيْءٍ أَوْ شِرَائِهِ مِنْ مَوْلَاهُ جَازَ ؛ لِأَنَّهُ لَا حَقَّ لِلْعَبْدِ فِي مَالِ مَوْلَاهُ وَكَانَتْ الْعُهْدَةُ عَلَيْهِ مَدْيُونًا كَانَ أَوْ غَيْرَ مَدْيُونٍ وَإِنْ أَقَرَّ بِالْقَبْضِ جَازَ إقْرَارُهُ ؛ لِأَنَّهُ يَصْلُحُ وَكِيلًا لِلْأَجْنَبِيِّ فِي قَبْضِ الدَّيْنِ مِنْ الْمَوْلَى وَيَصْلُحُ مُطَالِبًا لِلْمَوْلَى بِالثَّمَنِ إذَا بَاعَ مِنْهُ شَيْئًا مِنْ أَكْسَابِهِ وَعَلَيْهِ دَيْنٌ لِمُرَاعَاةِ حَقِّ غُرَمَائِهِ فَكَذَلِكَ لِمُرَاعَاةِ حَقِّ الْمُوَكِّلِ ، وَكَذَلِكَ لَوْ لَمْ يَدْفَعْ الْآمِرُ إلَى الْعَبْدِ شَيْئًا مِنْ الثَّمَنِ وَوَكَّلَهُ بِأَنْ يَشْتَرِيَ لَهُ مِنْ مَوْلَاهُ جَازَ شِرَاؤُهُ وَأَخَذَ الثَّمَنَ مِنْ الْآمِرِ وَدَفَعَهُ إلَى الْمَوْلَى ؛ لِأَنَّهُ فِي التَّوْكِيلِ بِالْمُعَامَلَةِ مَعَ الْمَوْلَى كَهُوَ فِي التَّوْكِيلِ بِالْمُعَامَلَةِ مَعَ أَجْنَبِيٍّ آخَرَ \"\n",
    "\n",
    "# Print each character with its Unicode code point\n",
    "matches = re.findall(r'[\\u0620-\\u064A][\\u064B-\\u0652]*', text)\n",
    "max = np.max([len(match) for match in matches])\n",
    "# print(max)\n",
    "# for match in matches:\n",
    "#     #if len(match) == 1:\n",
    "#         print(len(match), end =': ')\n",
    "#         for char in match:\n",
    "#             print(f\"{char}\", end = '')\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d6cf21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"بوَإِنْ كَانَ الْمَأْذُونُ هُوَ الْوَكِيلَ لِلْأَجْنَبِيِّ بِبَيْعِ شَيْءٍ أَوْ شِرَائِهِ مِنْ مَوْلَاهُ جَازَ ؛ لِأَنَّهُ لَا حَقَّ لِلْعَبْدِ فِي مَالِ مَوْلَاهُ وَكَانَتْ الْعُهْدَةُ عَلَيْهِ مَدْيُونًا كَانَ أَوْ غَيْرَ مَدْيُونٍ وَإِنْ أَقَرَّ بِالْقَبْضِ جَازَ إقْرَارُهُ ؛ لِأَنَّهُ يَصْلُحُ وَكِيلًا لِلْأَجْنَبِيِّ فِي قَبْضِ الدَّيْنِ مِنْ الْمَوْلَى وَيَصْلُحُ مُطَالِبًا لِلْمَوْلَى بِالثَّمَنِ إذَا بَاعَ مِنْهُ شَيْئًا مِنْ أَكْسَابِهِ وَعَلَيْهِ دَيْنٌ لِمُرَاعَاةِ حَقِّ غُرَمَائِهِ فَكَذَلِكَ لِمُرَاعَاةِ حَقِّ الْمُوَكِّلِ ، وَكَذَلِكَ لَوْ لَمْ يَدْفَعْ الْآمِرُ إلَى الْعَبْدِ شَيْئًا مِنْ الثَّمَنِ وَوَكَّلَهُ بِأَنْ يَشْتَرِيَ لَهُ مِنْ مَوْلَاهُ جَازَ شِرَاؤُهُ وَأَخَذَ الثَّمَنَ مِنْ الْآمِرِ وَدَفَعَهُ إلَى الْمَوْلَى ؛ لِأَنَّهُ فِي التَّوْكِيلِ بِالْمُعَامَلَةِ مَعَ الْمَوْلَى كَهُوَ فِي التَّوْكِيلِ بِالْمُعَامَلَةِ مَعَ أَجْنَبِيٍّ آخَرَ \"\n",
    "\n",
    "# Print each character with its Unicode code point\n",
    "matches = re.findall(r'[\\u0620-\\u064A][\\u064B-\\u0652]*', text)\n",
    "# for char in text:\n",
    "#     print(f\"{char}: U+{ord(char):04X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "77483f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode list: ['ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ', 'َّ', 'ُّ', 'ِّ', 'ًّ', 'ٌّ', 'ٍّ', '']\n",
      "Unicode to index: {'ً': 0, 'ٌ': 1, 'ٍ': 2, 'َ': 3, 'ُ': 4, 'ِ': 5, 'ّ': 6, 'ْ': 7, 'َّ': 8, 'ُّ': 9, 'ِّ': 10, 'ًّ': 11, 'ٌّ': 12, 'ٍّ': 13, '': 14}\n",
      "Index to Unicode: {0: 'ً', 1: 'ٌ', 2: 'ٍ', 3: 'َ', 4: 'ُ', 5: 'ِ', 6: 'ّ', 7: 'ْ', 8: 'َّ', 9: 'ُّ', 10: 'ِّ', 11: 'ًّ', 12: 'ٌّ', 13: 'ٍّ', 14: ''}\n"
     ]
    }
   ],
   "source": [
    "start = 0x064B\n",
    "end = 0x0652\n",
    "\n",
    "unicode_diacrits = [chr(code) for code in range(start, end + 1)]\n",
    "unicode_diacrits.append(chr(0x0651) + chr(0x064E)) \n",
    "unicode_diacrits.append(chr(0x0651) + chr(0x064F)) \n",
    "unicode_diacrits.append(chr(0x0651) + chr(0x0650)) \n",
    "unicode_diacrits.append(chr(0x0651) + chr(0x064B))\n",
    "unicode_diacrits.append(chr(0x0651) + chr(0x064C))\n",
    "unicode_diacrits.append(chr(0x0651) + chr(0x064D))\n",
    "unicode_diacrits.append(\"\")\n",
    "\n",
    "diacrits_to_index = {char: idx for idx, char in enumerate(unicode_diacrits)}\n",
    "index_to_diacrits  = {idx: char for idx, char in enumerate(unicode_diacrits)}\n",
    "\n",
    "print(\"Unicode list:\", unicode_diacrits)\n",
    "print(\"Unicode to index:\", diacrits_to_index)\n",
    "print(\"Index to Unicode:\", index_to_diacrits)\n",
    "\n",
    "letters_start = 0x0620\n",
    "letters_end = 0x064A\n",
    "\n",
    "unicode_letters = unicode_chars = [chr(code) for code in range(letters_start, letters_end + 1)]\n",
    "\n",
    "letters_to_index = {char: idx for idx, char in enumerate(unicode_letters)}\n",
    "index_to_letter = {idx: char for idx, char in enumerate(unicode_letters)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9fcbefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "414\n"
     ]
    }
   ],
   "source": [
    "matches = re.findall(r'[\\u0620-\\u064A][\\u064B-\\u0652]*', text)\n",
    "input = []\n",
    "output = []\n",
    "\n",
    "for match in matches:\n",
    "    if len(match) == 1:\n",
    "        input.append(letters_to_index[match[0]])\n",
    "        output.append(diacrits_to_index[''])\n",
    "    elif len(match) == 2:\n",
    "        input.append(letters_to_index[match[0]])\n",
    "        output.append(diacrits_to_index[match[1]])\n",
    "    elif len(match) == 3:\n",
    "        input.append(letters_to_index[match[0]])\n",
    "        output.append(diacrits_to_index[match[1]+match[2]])\n",
    "\n",
    "print(len(input))\n",
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9798d641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.6844\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the sequence labeling model\n",
    "class SingleMaskModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, mask_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, mask_classes)  # bidirectional doubles hidden_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                  # Convert input indices to embeddings\n",
    "        outputs, _ = self.rnn(x)               # Capture context with bidirectional LSTM\n",
    "        logits = self.fc(outputs)              # Predict diacritic class per letter\n",
    "        return logits\n",
    "\n",
    "# Sample vocab sizes (adjust as needed)\n",
    "vocab_size = len(unicode_letters)        # Number of letters in alphabet\n",
    "embedding_dim = len(unicode_letters)     # Embedding size for each letter\n",
    "hidden_dim = 64        # Hidden layer size in LSTM\n",
    "mask_classes = len(unicode_diacrits)     # Number of possible diacritics (including no diacritic)\n",
    "\n",
    "# Initialize model\n",
    "model = SingleMaskModel(vocab_size, embedding_dim, hidden_dim, mask_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example input: batch size 1, sequence length 3 (letters indices)\n",
    "input_tensor = torch.tensor([input])      # Example letters: [translate:ك], [translate:ت], [translate:ب]\n",
    "\n",
    "# Example target diacritics: sequence length 3 (mask indices per letter)\n",
    "target_tensor = torch.tensor([output])     # Diacritics for each letter\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "logits = model(input_tensor)                   # Forward pass: get predicted logits (batch, seq_len, mask_classes)\n",
    "logits_reshaped = logits.view(-1, mask_classes)    # Reshape for loss: (batch*seq_len, mask_classes)\n",
    "target_reshaped = target_tensor.view(-1)            # Flatten target: (batch*seq_len)\n",
    "loss = criterion(logits_reshaped, target_reshaped)  # Compute cross-entropy loss\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Training loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "447ac686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "بوَإِنْ كَانَ الْمَأْذُونُ هُوَ الْوَكِيلَ لِلْأَجْنَبِيِّ بِبَيْعِ شَيْءٍ أَوْ شِرَائِهِ مِنْ مَوْلَاهُ جَازَ ؛ لِأَنَّهُ لَا حَقَّ لِلْعَبْدِ فِي مَالِ مَوْلَاهُ وَكَانَتْ الْعُهْدَةُ عَلَيْهِ مَدْيُونًا كَانَ أَوْ غَيْرَ مَدْيُونٍ وَإِنْ أَقَرَّ بِالْقَبْضِ جَازَ إقْرَارُهُ ؛ لِأَنَّهُ يَصْلُحُ وَكِيلًا لِلْأَجْنَبِيِّ فِي قَبْضِ الدَّيْنِ مِنْ الْمَوْلَى وَيَصْلُحُ مُطَالِبًا لِلْمَوْلَى بِالثَّمَنِ إذَا بَاعَ مِنْهُ شَيْئًا مِنْ أَكْسَابِهِ وَعَلَيْهِ دَيْنٌ لِمُرَاعَاةِ حَقِّ غُرَمَائِهِ فَكَذَلِكَ لِمُرَاعَاةِ حَقِّ الْمُوَكِّلِ ، وَكَذَلِكَ لَوْ لَمْ يَدْفَعْ الْآمِرُ إلَى الْعَبْدِ شَيْئًا مِنْ الثَّمَنِ وَوَكَّلَهُ بِأَنْ يَشْتَرِيَ لَهُ مِنْ مَوْلَاهُ جَازَ شِرَاؤُهُ وَأَخَذَ الثَّمَنَ مِنْ الْآمِرِ وَدَفَعَهُ إلَى الْمَوْلَى ؛ لِأَنَّهُ فِي التَّوْكِيلِ بِالْمُعَامَلَةِ مَعَ الْمَوْلَى كَهُوَ فِي التَّوْكِيلِ بِالْمُعَامَلَةِ مَعَ أَجْنَبِيٍّ آخَرَ \n",
      "\n",
      "Reconstructed text:\n",
      "بوَإِنْكَانَالْمَأْذُونُهُوَالْوَكِيلَلِلْأَجْنَبِيِّبِبَيْعِشَيْءٍأَوْشِرَائِهِمِنْمَوْلَاهُجَازَلِأَنَّهُلَاحَقَّلِلْعَبْدِفِيمَالِمَوْلَاهُوَكَانَتْالْعُهْدَةُعَلَيْهِمَدْيُونًاكَانَأَوْغَيْرَمَدْيُونٍوَإِنْأَقَرَّبِالْقَبْضِجَازَإقْرَارُهُلِأَنَّهُيَصْلُحُوَكِيلًالِلْأَجْنَبِيِّفِيقَبْضِالدَّيْنِمِنْالْمَوْلَىوَيَصْلُحُمُطَالِبًالِلْمَوْلَىبِالثَّمَنِإذَابَاعَمِنْهُشَيْئًامِنْأَكْسَابِهِوَعَلَيْهِدَيْنٌلِمُرَاعَاةِحَقِّغُرَمَائِهِفَكَذَلِكَلِمُرَاعَاةِحَقِّالْمُوَكِّلِوَكَذَلِكَلَوْلَمْيَدْفَعْالْآمِرُإلَىالْعَبْدِشَيْئًامِنْالثَّمَنِوَوَكَّلَهُبِأَنْيَشْتَرِيَلَهُمِنْمَوْلَاهُجَازَشِرَاؤُهُوَأَخَذَالثَّمَنَمِنْالْآمِرِوَدَفَعَهُإلَىالْمَوْلَىلِأَنَّهُفِيالتَّوْكِيلِبِالْمُعَامَلَةِمَعَالْمَوْلَىكَهُوَفِيالتَّوْكِيلِبِالْمُعَامَلَةِمَعَأَجْنَبِيٍّآخَرَ\n",
      "\n",
      "Original has 414 letter+diacritic combinations\n",
      "Reconstructed has 414 letter+diacritic combinations\n",
      "\n",
      "Accuracy: 414/414 = 100.00%\n",
      "\n",
      "==================================================\n",
      "GROUND TRUTH:\n",
      "بوَإِنْكَانَالْمَأْذُونُهُوَالْوَكِيلَلِلْأَجْنَبِيِّبِبَيْعِشَيْءٍأَوْشِرَائِهِمِنْمَوْلَاهُجَازَلِأَنَّهُلَاحَقَّلِلْعَبْدِفِيمَالِمَوْلَاهُوَكَانَتْالْعُهْدَةُعَلَيْهِمَدْيُونًاكَانَأَوْغَيْرَمَدْيُونٍوَإِنْأَقَرَّبِالْقَبْضِجَازَإقْرَارُهُلِأَنَّهُيَصْلُحُوَكِيلًالِلْأَجْنَبِيِّفِيقَبْضِالدَّيْنِمِنْالْمَوْلَىوَيَصْلُحُمُطَالِبًالِلْمَوْلَىبِالثَّمَنِإذَابَاعَمِنْهُشَيْئًامِنْأَكْسَابِهِوَعَلَيْهِدَيْنٌلِمُرَاعَاةِحَقِّغُرَمَائِهِفَكَذَلِكَلِمُرَاعَاةِحَقِّالْمُوَكِّلِوَكَذَلِكَلَوْلَمْيَدْفَعْالْآمِرُإلَىالْعَبْدِشَيْئًامِنْالثَّمَنِوَوَكَّلَهُبِأَنْيَشْتَرِيَلَهُمِنْمَوْلَاهُجَازَشِرَاؤُهُوَأَخَذَالثَّمَنَمِنْالْآمِرِوَدَفَعَهُإلَىالْمَوْلَىلِأَنَّهُفِيالتَّوْكِيلِبِالْمُعَامَلَةِمَعَالْمَوْلَىكَهُوَفِيالتَّوْكِيلِبِالْمُعَامَلَةِمَعَأَجْنَبِيٍّآخَرَ\n",
      "\n",
      "MODEL PREDICTION:\n",
      "بوإنكانالِمٌأِذِونَهِوالِوكيلَلَلَأِجٌنَبَيَبَبَيَّعشيًّءِأِوشرًّائًّهًّمٌنَمٌولِاِهِجٌازًّلًّأِنِهِلًّاِحًّقًّلَلَعَبَدًّفًّيًّمٌالٌمٌولًّاهِوكانَتالعِهِدًّةِعلَيَّهًّمٌدًّيًّوناكانَأِوغًّيًّرمٌدُيُونَوإنَأقًّربالَقًّبَضٌجٌازًّإًّقًّرًّارهِلِأِنِهِيِصِلَحًّوكيٌّلِالَلَأِجٌنَبَيَفًّيًّقًّبضالَدًّيًّنِمٌنالَمٌولَىًّوًّيَّصلَحًّمًّطالَبالَلَمٌوَلَىًّبالَثًّمٌنَإذِاباعمٌنُهِشٌيًّئًّامٌنِأِكسّاِبهِوعلًّيَّهًّدًّيًّنْلَمٌراعاةِحًّقًّغًّرًّمٌائًّهًّفًّكِذِلًّكِلَمٌرًّاعاةِحًّقًّالَمٌوكلَوكذِلَكِلَوَلَمًّيَّدًّفًّعالَآًّمٌرإلٌىًّالٌعٌبَدًّشًّيًّئًّامٌنالَثمٌنَووكلَهِبَأنَيَشْتًّريَّلٌهٌمٌنَمٌولِاِهِجٌازًّشًّرًّاؤِهِوأِخذِالَثًّمٌنَمٌنالَآًّمٌرودًّفًّعًّهًّإلًّىًّالٌمٌولًّىًّلًّأًّنَهًّفًّيًّالَتًّوكيلَبالٌمٌعٌامٌلٌةِمٌعٌالٌمٌولًّىًّكِهِوًّفًّيًّالَتًّوكيلَبالٌمٌعٌامٌلٌةِمٌعٌأِجٌنَبَيَآخر\n",
      "==================================================\n",
      "\n",
      "Diacritic Accuracy: 81/414 = 19.57%\n",
      "\n",
      "First prediction errors:\n",
      "  Position 1: Letter 'و' - Expected 'وَ' Got 'و∅'\n",
      "  Position 2: Letter 'إ' - Expected 'إِ' Got 'إ∅'\n",
      "  Position 3: Letter 'ن' - Expected 'نْ' Got 'ن∅'\n",
      "  Position 4: Letter 'ك' - Expected 'كَ' Got 'ك∅'\n",
      "  Position 6: Letter 'ن' - Expected 'نَ' Got 'ن∅'\n",
      "  Position 8: Letter 'ل' - Expected 'لْ' Got 'لِ'\n",
      "  Position 9: Letter 'م' - Expected 'مَ' Got 'مٌ'\n",
      "  Position 10: Letter 'أ' - Expected 'أْ' Got 'أِ'\n",
      "  Position 11: Letter 'ذ' - Expected 'ذُ' Got 'ذِ'\n",
      "  Position 13: Letter 'ن' - Expected 'نُ' Got 'نَ'\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct diacritized text from predictions\n",
    "def reconstruct_text(input_indices, output_indices, index_to_letter, index_to_diacrits):\n",
    "    \"\"\"\n",
    "    Reconstruct diacritized text from letter and diacritic indices\n",
    "    \n",
    "    Args:\n",
    "        input_indices: List of letter indices\n",
    "        output_indices: List of diacritic indices\n",
    "        index_to_letter: Dictionary mapping index to letter\n",
    "        index_to_diacrits: Dictionary mapping index to diacritic\n",
    "    \n",
    "    Returns:\n",
    "        Reconstructed diacritized text string\n",
    "    \"\"\"\n",
    "    reconstructed = \"\"\n",
    "    for letter_idx, diacritic_idx in zip(input_indices, output_indices):\n",
    "        letter = index_to_letter[letter_idx]\n",
    "        diacritic = index_to_diacrits[diacritic_idx]\n",
    "        reconstructed += letter + diacritic\n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "# Test with your current data\n",
    "reconstructed_text = reconstruct_text(input, output, index_to_letter, index_to_diacrits)\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print(\"\\nReconstructed text:\")\n",
    "print(reconstructed_text)\n",
    "\n",
    "# Compare character by character\n",
    "def compare_texts(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Compare original and reconstructed texts character by character\n",
    "    \"\"\"\n",
    "    matches = 0\n",
    "    total = 0\n",
    "    mismatches = []\n",
    "    \n",
    "    # Extract only Arabic letters and diacritics from original\n",
    "    original_chars = re.findall(r'[\\u0620-\\u064A][\\u064B-\\u0652]*', original)\n",
    "    reconstructed_chars = list(reconstructed)\n",
    "    \n",
    "    # Reconstruct from individual characters\n",
    "    reconstructed_matches = []\n",
    "    temp = \"\"\n",
    "    for char in reconstructed:\n",
    "        if '\\u0620' <= char <= '\\u064A':  # Letter\n",
    "            if temp:\n",
    "                reconstructed_matches.append(temp)\n",
    "            temp = char\n",
    "        elif '\\u064B' <= char <= '\\u0652':  # Diacritic\n",
    "            temp += char\n",
    "    if temp:\n",
    "        reconstructed_matches.append(temp)\n",
    "    \n",
    "    print(f\"\\nOriginal has {len(original_chars)} letter+diacritic combinations\")\n",
    "    print(f\"Reconstructed has {len(reconstructed_matches)} letter+diacritic combinations\")\n",
    "    \n",
    "    # Compare each combination\n",
    "    for i, (orig, recon) in enumerate(zip(original_chars, reconstructed_matches)):\n",
    "        total += 1\n",
    "        if orig == recon:\n",
    "            matches += 1\n",
    "        else:\n",
    "            if len(mismatches) < 10:  # Show first 10 mismatches\n",
    "                mismatches.append((i, orig, recon))\n",
    "    \n",
    "    accuracy = (matches / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nAccuracy: {matches}/{total} = {accuracy:.2f}%\")\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\nFirst mismatches:\")\n",
    "        for idx, orig, recon in mismatches:\n",
    "            print(f\"  Position {idx}: Original '{orig}' vs Reconstructed '{recon}'\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "accuracy = compare_texts(text, reconstructed_text)\n",
    "\n",
    "# Get predictions from model and compare\n",
    "def compare_with_predictions(model, input_tensor, target_tensor, input_list, index_to_letter, index_to_diacrits):\n",
    "    \"\"\"\n",
    "    Get model predictions and compare with ground truth\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        predictions = torch.argmax(logits, dim=-1)  # Get predicted class indices\n",
    "    \n",
    "    predictions = predictions.squeeze().tolist()  # Convert to list\n",
    "    targets = target_tensor.squeeze().tolist()\n",
    "    \n",
    "    # Reconstruct both predicted and ground truth texts\n",
    "    predicted_text = reconstruct_text(input_list, predictions, index_to_letter, index_to_diacrits)\n",
    "    ground_truth_text = reconstruct_text(input_list, targets, index_to_letter, index_to_diacrits)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GROUND TRUTH:\")\n",
    "    print(ground_truth_text)\n",
    "    print(\"\\nMODEL PREDICTION:\")\n",
    "    print(predicted_text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = sum(1 for p, t in zip(predictions, targets) if p == t)\n",
    "    total = len(predictions)\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nDiacritic Accuracy: {correct}/{total} = {accuracy:.2f}%\")\n",
    "    \n",
    "    # Show mismatches\n",
    "    mismatches = []\n",
    "    for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "        if pred != target and len(mismatches) < 10:\n",
    "            letter = index_to_letter[input_list[i]]\n",
    "            pred_diacritic = index_to_diacrits[pred]\n",
    "            target_diacritic = index_to_diacrits[target]\n",
    "            mismatches.append((i, letter, target_diacritic, pred_diacritic))\n",
    "    \n",
    "    if mismatches:\n",
    "        print(\"\\nFirst prediction errors:\")\n",
    "        for idx, letter, target_dia, pred_dia in mismatches:\n",
    "            target_display = target_dia if target_dia else \"∅\"\n",
    "            pred_display = pred_dia if pred_dia else \"∅\"\n",
    "            print(f\"  Position {idx}: Letter '{letter}' - Expected '{letter}{target_display}' Got '{letter}{pred_display}'\")\n",
    "    \n",
    "    return accuracy, predictions\n",
    "\n",
    "# Compare model predictions with ground truth\n",
    "pred_accuracy, predictions = compare_with_predictions(\n",
    "    model, input_tensor, target_tensor, input, index_to_letter, index_to_diacrits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff38eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
