{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723cbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from utils.utils import create_data_pipeline\n",
    "from models.mohamed_ashraf.bilstm import BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e1c8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56058935",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/diacritic2id.pickle', 'rb') as f:\n",
    "    diacritic2idx = pickle.load(f)\n",
    "\n",
    "with open('utils/letter2idx.pickle', 'rb') as f:\n",
    "    letter2idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258275d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 38, Number of classes: 16\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(letter2idx)\n",
    "num_classes = len(diacritic2idx)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3b47936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    x_batch, y_batch, mask_batch = zip(*batch)\n",
    "    lengths_x = [len(x) for x in x_batch]\n",
    "    x_padded = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value=letter2idx['<PAD>'])\n",
    "    y_padded = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=diacritic2idx['<PAD>'])\n",
    "    mask_spadded = torch.nn.utils.rnn.pad_sequence(mask_batch, batch_first=True, padding_value=0)\n",
    "    return x_padded, y_padded, mask_spadded, torch.tensor(lengths_x, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff2682fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset,  test_loader= create_data_pipeline(\n",
    "    corpus_path='sample_test_no_diacritics.txt', \n",
    "    letter2idx=letter2idx, \n",
    "    diacritic2idx=diacritic2idx, \n",
    "    collate_fn=pad_collate_fn,\n",
    "    train=False, \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91541940",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(vocab_size=vocab_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "317e1a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115781/1611503649.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./models/mohamed_ashraf/bilstm.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(38, 128, padding_idx=13)\n",
       "  (bilstm): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"./models/mohamed_ashraf/bilstm.pth\", map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "# print(f\"Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "# print(f\"Validation Accuracy: {checkpoint['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a729254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_csv(model, test_loader, letter2idx, output_file='predictions.csv'):\n",
    "    model.eval()\n",
    "    space_idx = letter2idx.get(' ', None)\n",
    "    pad_idx = letter2idx.get('<PAD>', None)\n",
    "    \n",
    "    results = []\n",
    "    row_id = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, _, _, lengths in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            \n",
    "            outputs = model(batch_X, lengths)\n",
    "            preds = outputs.argmax(dim=-1)\n",
    "            \n",
    "            batch_X_cpu = batch_X.cpu().numpy()\n",
    "            preds_cpu = preds.cpu().numpy()\n",
    "            \n",
    "            for i in range(batch_X.size(0)):\n",
    "                seq_len = lengths[i].item()\n",
    "                \n",
    "                for j in range(seq_len):\n",
    "                    letter_idx = int(batch_X_cpu[i, j])\n",
    "                    diacritic_idx = int(preds_cpu[i, j])\n",
    "                    \n",
    "                    if letter_idx == pad_idx or letter_idx == space_idx:\n",
    "                        continue\n",
    "                    \n",
    "                    results.append({\n",
    "                        'ID': row_id,\n",
    "                        'label': diacritic_idx\n",
    "                    })\n",
    "                    row_id += 1\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fed1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = create_predictions_csv(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    letter2idx=letter2idx,\n",
    "    output_file='predictions.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be2eaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(gold_csv_path, predictions_csv_path):\n",
    "    gold_df = pd.read_csv(gold_csv_path)\n",
    "    pred_df = pd.read_csv(predictions_csv_path)\n",
    "    \n",
    "    print(f\"ðŸ“‚ Loaded files:\")\n",
    "    print(f\"  Gold standard: {len(gold_df):,} rows\")\n",
    "    print(f\"  Predictions: {len(pred_df):,} rows\")\n",
    "    print(f\"  Gold columns: {gold_df.columns.tolist()}\")\n",
    "    print(f\"  Pred columns: {pred_df.columns.tolist()}\")\n",
    "    \n",
    "    if len(gold_df) != len(pred_df):\n",
    "        print(f\"\\nâš ï¸  Warning: Row count mismatch!\")\n",
    "        print(f\"  Gold: {len(gold_df):,} | Predictions: {len(pred_df):,}\")\n",
    "        min_len = min(len(gold_df), len(pred_df))\n",
    "        print(f\"  Comparing first {min_len:,} rows only\")\n",
    "        gold_df = gold_df.head(min_len)\n",
    "        pred_df = pred_df.head(min_len)\n",
    "    \n",
    "    merged_df = pd.merge(gold_df, pred_df, on='ID', suffixes=('_gold', '_pred'))\n",
    "    \n",
    "    correct = (merged_df['label_gold'] == merged_df['label_pred']).sum()\n",
    "    total = len(merged_df)\n",
    "    accuracy = (correct / total) * 100\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸ“Š Accuracy Results\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"  Total samples: {total:,}\")\n",
    "    print(f\"  Correct predictions: {correct:,}\")\n",
    "    print(f\"  Incorrect predictions: {total - correct:,}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}%\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Per-Label Analysis:\")\n",
    "    unique_labels = sorted(merged_df['label_gold'].unique())\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_mask = merged_df['label_gold'] == label\n",
    "        label_total = label_mask.sum()\n",
    "        label_correct = ((merged_df['label_gold'] == label) & \n",
    "                         (merged_df['label_pred'] == label)).sum()\n",
    "        label_acc = (label_correct / label_total * 100) if label_total > 0 else 0\n",
    "        print(f\"  Label {label}: {label_correct:4d}/{label_total:4d} correct ({label_acc:6.2f}%)\")\n",
    "    \n",
    "    return accuracy, merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "769f72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loaded files:\n",
      "  Gold standard: 182 rows\n",
      "  Predictions: 182 rows\n",
      "  Gold columns: ['ID', 'label']\n",
      "  Pred columns: ['ID', 'label']\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š Accuracy Results\n",
      "============================================================\n",
      "  Total samples: 182\n",
      "  Correct predictions: 171\n",
      "  Incorrect predictions: 11\n",
      "  Accuracy: 93.9560%\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Per-Label Analysis:\n",
      "  Label 0:   58/  61 correct ( 95.08%)\n",
      "  Label 1:    3/   3 correct (100.00%)\n",
      "  Label 2:   11/  14 correct ( 78.57%)\n",
      "  Label 4:   22/  25 correct ( 88.00%)\n",
      "  Label 5:    1/   1 correct (100.00%)\n",
      "  Label 6:   35/  36 correct ( 97.22%)\n",
      "  Label 8:    5/   5 correct (100.00%)\n",
      "  Label 10:    2/   2 correct (100.00%)\n",
      "  Label 12:    0/   1 correct (  0.00%)\n",
      "  Label 14:   34/  34 correct (100.00%)\n"
     ]
    }
   ],
   "source": [
    "accuracy, comparison_df = calculate_accuracy(\n",
    "    gold_csv_path='sample_test_set_gold.csv',\n",
    "    predictions_csv_path='predictions.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
